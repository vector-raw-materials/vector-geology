{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\nfrom pyvista import set_plot_theme\nset_plot_theme('document')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Modeling and Propagation of Petrophysical Data for Mining Exploration (2/3)\n**Cleaning and Filling the Gaps**\n\nBarcelona 25/09/24\nGEO3BCN\nManuel David Soto, Juan Alcalde, Adri\u00e0 Hern\u00e0ndez-Pineda, Ram\u00f3n Carbonel\n\n## Introduction\n\nThe dispersion and scarcity of petrophysical data are well-known challenges in the mining sector. These issues are primarily driven by economic factors, but also by geological factors such as sedimentary cover, weathering, erosion, or the depth of targets, geotechnical issues (e.g., slope or borehole stability), and even technical limitations or availability that have been resolved in other industries (for instance, sonic logs were not previously acquired due to a lack of interest in velocity field data).\n\nTo address the challenge of sparse and incomplete petrophysical data in the mining sector, we have developed three Jupyter notebooks that tackle these issues through a suite of open-source Python tools. These tools support researchers in the initial exploration, visualization, and integration of data from diverse sources, filling gaps caused by technical limitations and ultimately enabling the complete modeling of missing properties (through standard and more advanced ML-based models). We applied these tools to both recently acquired and legacy petrophysical data of two cores northwest of Collinstown (County Westmeath, Province of Leinster), Ireland, located 26 km west-northwest of the Navan mine. However, these tools are adaptable and applicable to mining data from any location.\n\nAfter the EDA and integration of the petrophysical dataset of Collinstown (notebook 1/3), this second notebook gathers different tasks that can be grouped into what is called, in data science, data mining. These tasks are:\n\n* Record the positions of the original NaNs.\n* Record the positions and the values of the anomalies to be deleted.\n* Delete the anomalies.\n* Fill out all NaNs, both original and those resulting from deleting the anomalies, using different means (imputations, empirical formulas, simple ML models).\n* Compare the effectiveness of the different filling gap methods.\n* Use the better option to fill in the gaps and deliver the corrected petrophysical data for further investigation.\n\nAs with the previous notebook, these tasks are performed with open-source Python tools that are easily accessible by any researcher through a Python installation connected to the Internet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variables\nThe dataset used in this notebook is the 'features' dataset from the previous notebook (1/3). It contains the modelable petrophysical features with their respective anomalies. 'Hole' (text object) and 'Len' (float) variables are for reference, 'Form' (text object) is a categorical variable representing the major formations:\n\n+------+------------------------------------------------+--------+\n| Name | Explanation                                    | Unit   |\n+======+================================================+========+\n| Hole | Hole name                                      | -      |\n+------+------------------------------------------------+--------+\n| From | Top of the sample                              | m      |\n+------+------------------------------------------------+--------+\n| Len  | Length of the core sample                      | cm     |\n+------+------------------------------------------------+--------+\n| Den  | Density                                        | g/cm\u00b3  |\n+------+------------------------------------------------+--------+\n| Vp   | Compressional velocity                         | m/s    |\n+------+------------------------------------------------+--------+\n| Vs   | Shear velocity                                 | m/s    |\n+------+------------------------------------------------+--------+\n| Mag  | Magnetic susceptibility                        | -      |\n+------+------------------------------------------------+--------+\n| Ip   | Chargeability or induced polarization          | mv/V   |\n+------+------------------------------------------------+--------+\n| Res  | Resistivity                                    | ohm\u00b7m  |\n+------+------------------------------------------------+--------+\n| Form | Major formations or zone along the hole        | -      |\n+------+------------------------------------------------+--------+\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries\nThe following are the Python libraries used along this notebook. PSL are Python Standard Libraries, UDL are User Defined Libraries, and PEL are Python External Libraries:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PLS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\nimport warnings\n\n# UDL\nfrom vector_geology import basic_stat, geo\n\n# PEL- Basic\nimport numpy as np\nimport pandas as pd\nfrom tabulate import tabulate\n\n# PEL - Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PEL - Data selection, transformation, preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# PEL - Imputer\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# PEL- ML algorithms\nfrom scipy.optimize import curve_fit\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn import tree\nimport xgboost as xgb\n\n# PEL - Metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Settings\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seed of random process\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = 123\n\n# Warning suppression\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Features data load\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features = pd.read_csv('Output/features.csv', index_col=0)\nfeatures.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Columns in the features dataframe\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Features Cleaning\nThe plots below show the anomalous values of the four affected variables. Then, along the section, we will register the position of the anomalies and then delete them, according to the findings of the previous notebook (1/3):\n\n+------+--------------------------------------------------------------+\n| Feat.| Deliting Anomalies                                           |\n+======+==============================================================+\n| Den  | Values above 4 g/cm\u00b3 are related to bad measurements in core |\n|      | samples longer than 11.2 cm.                                 |\n+------+--------------------------------------------------------------+\n| Vp   | Values below 3000 m/s are related to fractures in the core   |\n|      | samples.                                                     |\n+------+--------------------------------------------------------------+\n| Vs   | Values less than 1000 m/s are also related to core fractures.|\n+------+--------------------------------------------------------------+\n| Mag  | The anomaly of 93.9 is above the range of the measuring      |\n|      | equipment.                                                   |\n+------+--------------------------------------------------------------+\n| Ip   | We see no reasons to discard outliers.                       |\n+------+--------------------------------------------------------------+\n| Res  | We see no reasons to discard outliers.                       |\n+------+--------------------------------------------------------------+\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bad data areas in Vp, Vs, Mag vs. Den\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 5))\n\n# len vs. Den\nplt.subplot(131)\nplt.scatter(features.Vp, features.Den, alpha=0.65, edgecolors='k', c='skyblue')\nplt.axhspan(4, 5.25, color='r', alpha=0.2)\nplt.axvspan(500, 3000, color='r', alpha=0.2)\nplt.text(1100, 4.7, 'Bad Data Area')\nplt.axis([500, 7000, 2, 5.25])\nplt.xlabel('Vp (m/s)')\nplt.ylabel('Den (g/cm3)')\nplt.grid()\n\n# Den vs. Vs bad data area\nplt.subplot(132)\nplt.scatter(features.Vs, features.Den, alpha=0.65, edgecolors='k', c='skyblue')\nplt.axhspan(4, 5.25, color='r', alpha=0.2)\nplt.axvspan(0, 1000, color='r', alpha=0.2)\nplt.text(75, 4.7, 'Bad Data Area')\nplt.axis([0, 4000, 2, 5.25])\nplt.xlabel('Vs (m/s)')\nplt.ylabel('Den (g/cm3)')\nplt.grid()\n\n# Den vs. Vs bad data area\nplt.subplot(133)\nplt.scatter(features.Mag, features.Den, alpha=0.65, edgecolors='k', c='skyblue')\nplt.axvspan(20, 100, color='r', alpha=0.2)\nplt.axhspan(4, 5.25, color='r', alpha=0.2)\nplt.text(2, 4.7, 'Bad Data Area')\nplt.axis([-5, 100, 2, 5.25])\nplt.xlabel('Magnetic Susceptibility')\nplt.ylabel('Den (g/cm3)')\nplt.grid()\n\nplt.suptitle('Variable with Anomalies in the Keywells')\n\nplt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NaN and Anomalies Preservation\nThe folowing dataframes preserves the location of original NaN (1) and anomalies (2):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nan_ano_loc = features.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fill the datafreme with 1 and 2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for column in features.columns:\n    nan_ano_loc[column] = np.where(pd.isna(features[column]), 1, 0)\n\nnan_ano_loc.Den = np.where(features.Den > 4, 2, 0)\nnan_ano_loc.Vp = np.where(features.Vp < 3000, 2, 0)\nnan_ano_loc.Vs = np.where(features.Vs < 1000, 2, 0)\nnan_ano_loc.Mag = np.where(features.Mag > 20, 2, 0)\nnan_ano_loc.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nan_ano_loc.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the locations of nans and anomalies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nan_ano_loc.to_csv('Output/nan_ano_loc.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preservation of the anomalies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "anomalies = features[features.Den > 4]\nanomalies = pd.concat([anomalies, features[features.Vp < 3000]])\nanomalies = pd.concat([anomalies, features[features.Vs < 1000]])\nanomalies = pd.concat([anomalies, features[features.Mag > 2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "anomalies = anomalies.drop_duplicates().sort_index()\nanomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Number and % of anomalies by boreholes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Total anomalies:', len(anomalies), '({:.1f}%)'.format(100 * len(anomalies) / (features.shape[0] * features.shape[1])))\nanomalies.Hole.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the anomalies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "anomalies.to_csv('Output/anomalies.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rows with anomalies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "list(anomalies.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anomalies Deletion\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "features2, new features dataframe without anomalies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2 = features.copy()\n\nfeatures2['Den'] = np.where(features.Den > 4, np.nan, features.Den)\nfeatures2['Vp'] = np.where(features.Vp < 3000, np.nan, features.Vp)\nfeatures2['Vs'] = np.where(features.Vs < 1000, np.nan, features.Vs)\nfeatures2['Mag'] = np.where(features.Mag > 20, np.nan, features.Mag)\n\nfeatures2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boxplot of each feature without anomalies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.plot.box(subplots=True, grid=False, figsize=(12, 7), layout=(3, 4), flierprops={\"marker\": \".\"})\nplt.suptitle('Features Without Anomalies')\nplt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save features without anomalies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.to_csv('Output/features2.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NaNs in the Features\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Original NaNs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Total original NaNs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "% of original NaNs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features.isna().sum() / len(features) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Total % original and new NaNs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('% NaN in Features:', round(100 * features.isna().sum().sum() / (features.shape[0] * features.shape[1]), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Original and new NaNs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Total original and new NaNs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Total % original and new NaNs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('% NaN in features2:', round(100 * features2.isna().sum().sum() / (features2.shape[0] * features2.shape[1]), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "% Total original and new NaNs by feature\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.isna().sum() / len(features) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bar plot of NaNs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.isna().sum().plot.bar(figsize=(8, 3), color='r', ylabel='Count', title='All Features NaN', label='Without anomalies', legend=True)\nfeatures.isna().sum().plot.bar(figsize=(8, 3), color='b', label='With anomalies', legend=True, grid=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bar plot of NaNs (%)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(features2.isna().sum() / len(features) * 100).plot.bar(figsize=(8, 3), color='r', label='Without anomalies', legend=True)\n(features.isna().sum() / len(features) * 100).plot.bar(figsize=(8, 3), color='b', label='With anomalies', legend=True,\n                                                       title='All Features NaN', ylabel='%', grid=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Total rows at least one with NaN and their indexes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_nan_index = list(features2[pd.isna(features2).any(axis=1)].index)\nprint('Total rows with at least one NaN:', len(total_nan_index), '\\n', '\\n', 'Rows indexes:')\ntotal_nan_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filling the holes\nImputations Evaluation\n++++++++++++++++++++++\nImputation refers to the alternative process of filling in all the missing values (NaN) or gaps in a dataset. Instead of removing rows or columns with missing values, those values are replaced (imputed) with estimated values based on information available in the entire dataset (in all variables). This is typically done when you want to preserve the other values in the rows where the NaNs are, and when the missing values for each variable or column do not exceed 25%. \n\nSince multiple imputation methods are available, it's important to evaluate each one and select the method that yields the best results, based on metrics such as the sum of residuals, MAE, MSE, RMSE, and R\u00b2 (formulas of these metrics in the annex). In this case, we tested six imputation methods: \n\n* Simple mean (mean)\n* Simple K-Means (knn)\n* Iterative Impute (ii)\n* Normalized mean (nmean)\n* Normalized K-Means (nknn)\n* Normalized Iterative Imputer (nii)\n\nTo compare these methods, we first created fictitious gaps in the Vp values (just this variable due to its importance, and for simplicity and speed) of a non-NaN dataset to compare them later the real values of Vp against the imputed. By doing so, we can compute each method's metrics and determine which performs best. In addition to these metrics, at the end of the section, there is a plot with the resulting values of each imputer method with respect to the real values, showing less dispersion towards the top of the plot.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Non-NaN dataframe to test the imputation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2_num = features2[['From', 'Den', 'Vp', 'Vs', 'Mag', 'Ip', 'Res']]\nfeatures2_num_nonan = features2_num.dropna()\nfeatures2_num_nonan.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generation of random mask for the fictitious NaNs or gaps\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(seed)\nmissing_rate = 0.2  # Porcentaje de valores faltantes\nVp_mask = np.random.rand(features2_num_nonan.shape[0]) < missing_rate\nVp_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "New gaps in Vp of the Non-NaN dataframe \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2_new_missing = features2_num_nonan.copy()\nfeatures2_new_missing.loc[Vp_mask, 'Vp'] = np.nan\nfeatures2_new_missing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At index 328 the original Vp is\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features.loc[328]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "True Vp values\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "true_vp = features2_num_nonan[Vp_mask].Vp.reset_index(drop=True)\ntrue_vp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation of the imputers\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# List of imputers\nimputers = []\n\n# Initialize imputers\nmean_imputer = SimpleImputer(strategy='mean')\nii_imputer = IterativeImputer(max_iter=10, random_state=seed)\nknn_imputer = KNNImputer(n_neighbors=5)\n\n# Normalization\nscaler = StandardScaler()\nfeatures2_new_missing_scaled = scaler.fit_transform(features2_new_missing)\n\n# Append of imputer (name, norm, imputer, data)\nimputers.append(('mean', 'no', mean_imputer, features2_new_missing))\nimputers.append(('ii', 'no', ii_imputer, features2_new_missing))\nimputers.append(('knn', 'no', knn_imputer, features2_new_missing))\nimputers.append(('nmean', 'yes', mean_imputer, features2_new_missing_scaled))\nimputers.append(('nii', 'yes', ii_imputer, features2_new_missing_scaled))\nimputers.append(('nknn', 'yes', knn_imputer, features2_new_missing_scaled))\n\n# Results table\nheaders = ['Imputer', 'Normalization', 'Sum Residual', 'MAE', 'MSE', 'RMSE', 'R2']\nrows = []\n\n# Loop over the imputers\nfor name, norm, imputer, data in imputers:\n    # Impute the data\n    imputed_data = imputer.fit_transform(data)\n\n    # Reverse the normalization if data was normalized\n    if norm == 'yes':\n        imputed_data = scaler.inverse_transform(imputed_data)\n\n    # Convert the array to dataframe\n    imputed_data_df = pd.DataFrame(imputed_data, columns=features2_new_missing.columns)\n    imputed_vp = imputed_data_df.loc[Vp_mask, 'Vp'].reset_index(drop=True)\n\n    # Calculate residuals and metrics\n    residual = sum(true_vp - imputed_vp)\n    mae = mean_absolute_error(true_vp, imputed_vp)\n    mse = mean_squared_error(true_vp, imputed_vp)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(true_vp, imputed_vp)\n\n    # Create a dataframe for imputer using globals()\n    globals()[f'imputer_{name}'] = imputed_data_df.copy()\n\n    # Append results\n    rows.append([name, norm, residual, mae, mse, rmse, r2])\n\n# Print the results in a formatted table\n\nprint('Seed:', seed)\n\nprint(tabulate(rows, headers=headers, tablefmt=\"fancy_outline\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "imputer_nii.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot of the real Vp vs. the imputed Vp\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\nplt.scatter(true_vp, true_vp, color='k', label='True Values')\nplt.scatter(true_vp, imputer_mean[Vp_mask].Vp, label='Mean (mean)')\nplt.scatter(true_vp, imputer_nmean[Vp_mask].Vp, label='Normalized Mean (nmean)')\nplt.scatter(true_vp, imputer_ii[Vp_mask].Vp, label='Iterative (ii)')\nplt.scatter(true_vp, imputer_nii[Vp_mask].Vp, label='Normalized (nii)')\nplt.scatter(true_vp, imputer_knn[Vp_mask].Vp, label='K-Mean Neighbor (knn)')\nplt.scatter(true_vp, imputer_nknn[Vp_mask].Vp, label='Normalized K-Mean Neighbor (nknn)')\nplt.xlabel('True Vp (m/s)')\nplt.ylabel('Imputed Vp (m/s)')\nplt.title('Real Vp vs. Imputed Vp')\nplt.axis([4000, 6750, 4000, 6750])\nplt.legend()\nplt.grid()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best Imputation\n +++++++++++++++  \nBy a closer examination of the metrics, we can conclude that the normalized Iterative Imputer (nii), also called MICE (Multiple Imputation by Chained Equations), performs the best (or least poorly), particularly for the Vp variable, with the mean of the imputed values increasing just 1.2 % with respect to the mean of the real values. Consequently, we used the nii imputer to fill the gaps across all variables and saved the updated dataset in a new dataframe (features3).\n\nThe multiplot of this section shows that the imputation process maintains the shape of the boxplot of the variables from which the anomalies have been removed. In other words, this multiplot of the imputed dataset (features3) is almost identical to the multiplot of features2 in section 7.2.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalized interactive imputation of features2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Data normalization\nscaler = StandardScaler()\nfeatures2_scaled = scaler.fit_transform(features2_num)\n\n# Imputation of normalized data\nnii_features3_array = ii_imputer.fit_transform(features2_scaled)\n\n# Reverse normalization and new imputed dataframe\nfeatures3 = pd.DataFrame(scaler.inverse_transform(nii_features3_array), columns=features2_num.columns)\n\nfeatures3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features3.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boxplot of each imputed feature\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features3.plot.box(subplots=True, grid=False, figsize=(12, 7), layout=(3, 4), flierprops={\"marker\": \".\"})\nplt.suptitle('Imputed Features')\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy reference variable to features3\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features3[['Hole', 'Len', 'Form']] = features2[['Hole', 'Len', 'Form']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Order of the variables\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features3_series = list(features2.columns)\nfeatures3_series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reordering features3\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features3 = features3[features3_series]\nfeatures3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save features3\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features3.to_csv('Output/features3.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Indixes of Vp values in features2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.Vp[~features2.Vp.isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Indixes of Vp NaNs in features2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.Vp[features2.Vp.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Vp_nan_index = list(features2.Vp[features2.Vp.isna()].index)\nVp_nan_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Original Vp without anomalies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.Vp.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imputed Vp\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features3.Vp.iloc[Vp_nan_index].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "% of increase of the mean of the imputed Vp compared with the real Vp\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vp_change_den = 100 * (features3.Vp.iloc[Vp_nan_index].mean() - features2.Vp.mean()) / features2.Vp.mean()\nprint('Increase of the mean:', '{:.1f} %'.format(vp_change_den))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "nii Vp for comparison\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nii_vp = features3.Vp.iloc[Vp_nan_index]\nnii_vp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating Vp\nThe calculation of a single variable such as Vp (again, just this variable due to its importance, for simplicity and speed) allows us to try and evaluate different methods for filling the gaps, from empirical formulas to the simplest Machine Learning (ML) algorithms.\n\n### Vp by Gardner\nThe Gardner's formula is a well-known empirical formula that allows us to transform the density (Den) into compressional velocity (Vp):\n\n\\begin{align}Vp = \\alpha * \\phi ^ {\\beta}\\end{align}\n\nWhere\n\n$\\phi$ is the density and the standard coefficients (reference: https://wiki.seg.org/wiki/Dictionary:Gardner%E2%80%99s_equation) are:\n\n\\begin{align}\\alpha \\approx 4348\\end{align}\n\n\\begin{align}\\beta = 0.25\\end{align}\n\nContrary to the expected increasing trend between Den and Vp, the plot below shows an anomalous vertical trend, where multiple Vp values are associated with a single Den value (around 2.7). This anomalous trend results in a negative R\u00b2 (-0.021, the worst so far) when Vp is calculated using all values, and this score improves slightly when we filter out the pairs that fall outside the expected increasing trend. This is confirmed by the low covariance presented by the Den-Vp pair in the covariance matrix of this section (low correlation coefficient in the previous notebook 1/3).\n%%\nPlot of Den vs. Vp\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.grid()\nplt.scatter(features2.Den, features2.Vp, alpha=0.65, edgecolors='k')\nplt.xlabel('Den (g/cm$^3$)')\nplt.ylabel('Vp (m/s)')\nplt.title('Data Without Anomalies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2.select_dtypes(include=['number'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Covariance of the features2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vari_mat = features2.select_dtypes(include=['number']).cov()\nvari_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vari_mat.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Covariance matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sns.heatmap(vari_mat, vmin=-500, vmax=500, center=0, linewidths=.1, cmap='seismic_r', annot=True)\nplt.title('Covariance Matrix')\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gardner Vp in features2, with the standard alpha of 4348\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2['VpG'] = 4348 * (features2.Den) ** 0.25\nfeatures2['VpG']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop NaNs for metrics calculation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2_nonan = features2.dropna()\nfeatures2_nonan.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metrics of the Gardner calculation with all values\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vpg_metrics = basic_stat.metrics(features2_nonan.Vp, features2_nonan.VpG)\n\nprint(\"Metrics for Gardner:\\n\")\nprint(vpg_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metrics of the Gardner calculation with filtered values\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vpg_metrics2 = basic_stat.metrics(\n    features2_nonan.Vp[(features2_nonan.Vp > 4000) & (features2_nonan.Vp < 6000)],\n    features2_nonan.VpG[(features2_nonan.Vp > 4000) & (features2_nonan.Vp < 6000)]\n)\n\nprint(\"Metrics for Gardner with Filtered Data Vp):\\n\")\nprint(vpg_metrics2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gardner Vp for comparison\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gard_vp = features2.VpG.iloc[Vp_nan_index]\ngard_vp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simpliest ML models for Vp\n\nMachine Learning (ML) algorithms offer a wide range of options to calculate missing values of a single variable. From the simplest and most well-known, such as linear regression with an independent variable, to the most complex. Below we use the simplest ML algorithms and their respective metrics, applied again to predict Vp and fill its gaps. But first, we split the non-NaN portion of the filtered data (features2) for training and testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2_num_nonan.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Target from filtered features2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The target or objective of the model is the Vp\ntarget = features2_num_nonan.Vp[(features2_num_nonan.Vp > 4000) & (features2_num_nonan.Vp < 6000)]\nprint(target.shape)\ntarget.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filtered density is the independent feature to compute Vp, the target\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "features2_den = features2_num_nonan.Den[(features2_num_nonan.Vp > 4000) & (features2_num_nonan.Vp < 6000)]\nfeatures2_den.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split and shape of data for training and testing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features2_den, target, test_size=0.2, random_state=seed)\n\nX_train = np.array(X_train).reshape(-1, 1)\nX_test = np.array(X_test).reshape(-1, 1)\ny_train = np.array(y_train).reshape(-1, 1)\ny_test = np.array(y_test).reshape(-1, 1)\n\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)\n\nprint('X_test shape:', X_test.shape)\nprint('y_test shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple Linear Regression\n\nThe linear regression is perhaps one of the simplest ML algorithms, it allows us to define a simple formula to compute Vp from Den, the only feature. The regression using all the data gave a negative $R^2$ (the predictions are worse than if it had simply predicted the mean), while applying a filter in the input data(4000 < Vp < 6000) resulted in a positive $R^2$, although a very small one (0.003).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear Regression\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Model training and prediction\nlr_model = LinearRegression().fit(X_train, y_train)\nlr_predict = lr_model.predict(np.array(X_test))\n\n# Coheficients\ncoef = lr_model.coef_.item()\ninter = lr_model.intercept_.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metric of the linear regression with filtered data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr_metrics = basic_stat.metrics(y_test, lr_predict)\n\nprint(\"Metrics for Linear Regressor:\\n\")\nprint(lr_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot of filtered data and regression\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_test, y_test, label='Real data', alpha=0.65, edgecolors='k')\nplt.plot(X_test, lr_predict, c='r', label='Prediction')\nplt.title(f\"Vp = {coef:.1f} * den + {inter:.1f}\")\nplt.suptitle('Linear Regression')\nplt.xlabel('Density (g/cm$^3$)')\nplt.ylabel('Vp (m/s)')\nplt.grid()\nplt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "lr Vp for comparison\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#  Density of the Vp calculations\nden_feature = np.array(features2.Den.iloc[Vp_nan_index]).reshape(-1, 1)\n\nlr_vp = lr_model.predict(den_feature)\nlr_vp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Non Linear Fit\nAlthough the non_linear fit is more powerful since it can fit the alpha and beta coefficients of a non-linear curve of density to Vp, the $R^2$ metric (0.004) does not differ much from the linear regression with the filtered data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reshaping the filtered data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train).flatten()\ny_train = np.array(y_train).flatten()\n\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gardner function for double fitting, for alpha and beta\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gardner_model(X_train, alpha, beta):\n    return alpha * X_train ** beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model fit\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "popt, pcov = curve_fit(gardner_model, X_train, y_train, p0=[0, 0])  # p0 is the initial estimation of alpha y beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Coheficients and prediction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpha, beta = popt\n\nprint('Alpha:', alpha)\nprint('Beta:', beta)\n\nnlf_predict = gardner_model(X_test, alpha, beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metric of the non linear fit with filtered data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nlf_metrics = basic_stat.metrics(y_test, nlf_predict)\n\nprint(\"Metrics for Non Linear Fit:\\n\")\nprint(nlf_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot of filtered data and NLF equation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_test, y_test, label='Real data')\nplt.plot(X_test, nlf_predict, c='r', label='Prediction')\nplt.title(f\"Vp = {alpha:.2f} * den ** {beta:.2f}\")\nplt.xlabel(\"Density (g/cm3)\")\nplt.suptitle('Non Linear Fit')\nplt.ylabel(\"Vp (m/s)\")\nplt.grid()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "nlf Vp for comparison\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nlf_vp = gardner_model(den_feature, alpha, beta)\nnlf_vp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vps Comparison\n--------------    \nThe real Vp values allow us to evaluate the quality of different procedures that estimate Vp, such as different imputation algorithms, the Gardner empirical formula, and the simpliest ML regression model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Datos y t\u00edtulos a utilizar en el bucle\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9, 15))\n\n# Histograms\nplt.subplot(7, 1, 1)\nplt.grid(zorder=2)\nplt.hist(features2.Vp, bins=20, zorder=3)\nplt.xlim(2500, 7000)\nplt.xlabel('m/s')\nplt.title('Real Vp')\n\nplt.subplot(7, 1, 2)\nbox = plt.boxplot(features2.Vp.dropna(), vert=False, showmeans=True, meanline=True, patch_artist=False)\nmean_line = box['means'][0]  # Obtener la l\u00ednea de la media\nmedian_line = box['medians'][0]  # Obtener la l\u00ednea de la mediana\nplt.legend([mean_line, median_line], ['Mean', 'Median'], loc='upper left')\nplt.xlim(2500, 7000)\nplt.title('Real Vp')\nplt.xlabel('m/s')\nplt.grid()\n\nplt.subplot(7, 1, 3)\nplt.grid(zorder=2)\nplt.hist(gard_vp, label='Gardner Vp', zorder=4)\nplt.hist(nii_vp, label='NII Imputer Vp', zorder=7)\nplt.hist(lr_vp, label='LR Vp', zorder=9)\nplt.hist(nlf_vp, label='NLF Vp', zorder=10)\nplt.xlim(2500, 7000)\nplt.xlabel('m/s')\nplt.legend()\nplt.title('Estimated Vps')\n\n# Boxplots\ndata_list = [gard_vp, nii_vp, lr_vp, nlf_vp]\ntitles = ['Gardner Vp', 'NII Inputer Vp', 'LR Vp', 'NLF Vp']\nmeans = [np.mean(features2.Vp.dropna()), np.mean(gard_vp), np.mean(nii_vp), np.mean(lr_vp), np.mean(nlf_vp)]\n\nfor i in range(len(data_list)):\n    plt.subplot(7, 1, i + 4)\n    plt.boxplot(data_list[i], vert=False, showmeans=True, meanline=True)\n    vp_change = 100 * (data_list[i].mean() - features2.Vp.mean()) / features2.Vp.mean()\n    plt.text(means[i] - 500, 1.25, 'Mean: {:.0f}, % change: {:.1f}'.format(means[i], vp_change))\n    plt.xlim(2500, 7000)\n    plt.title(titles[i])\n    plt.xlabel('m/s')\n    plt.grid()\n\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations\n\nHere are some observations related to the tasks covered in this notebook:\n\n* Anomalies represent 0.5% (18) of the data, and after removing them we reach a total of 4.3% of NaN (142).\n* The metrics favor the nii imputer. However, the knn, ii, and nknn imputer provided similar results. In the end, the dataset without anomalies (features2) was filled with the nii imputer (features3) because it was the best option.\n* In addition to the correlation matrix (described in the previous notebook 1/3), it is important to explore the covariance matrix because it provides valuable information regarding the scale-dependent relationships between variables, reflecting how changes in one variable are associated with changes in another in terms of their actual units.\n* The poor correlation (low covariance) between Den and Vp was not realized until Gardner's Vp was calculated. This gave negative $R^2$, even after filtering part of the data outside the increasing trend.\n* Filtering the Den and Vp values outside the increasing trend allows us to obtain positive but very small $R^2$ for the LR and the NLF ML algorithms.\n\n## Next Step\n\nThe next and last notebook (3/3), after the imputation, is going to focus on the prediction of an entire variable (GR), missing in one of the available boreholes.\n\n## Reference\n\n[Gardner's Equation](https://wiki.seg.org/wiki/Dictionary:Gardner%E2%80%99s_equation)\n\n## Annex - Regression Metrics\n\nThese metrics help you understand different aspects of prediction accuracy and are critical for evaluating and comparing regression models. The main metrics are:\n\n#### Sum of Residuals\n\nThe Sum of Residuals is the sum of errors between the predicted and the actual values. Ideally, it should add up to 0.\n\n\\begin{align}\\text{Sum of Residuals} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\end{align}\n\nWhere:\n\n- $y_i$ is the real value.\n- $\\hat{y}_i$ is the predicted value.\n\n#### MAE - Mean Absolute Error\n\nThe MAE measures the average of the absolute errors between the predicted and the actual values. It is an intuitive and easy-to-interpret metric.\n\n\\begin{align}\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\end{align}\n\nWhere:\n\n- $n$ is the number of observations.\n\n#### MSE - Mean Squared Error\n\nThe MSE measures the average of the squared errors between the predicted values and the actual values. It penalizes large errors more than the MAE.\n\n\\begin{align}\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\end{align}\n\n#### RMSE - Root Mean Squared Error\n\nThe RMSE is the square root of the MSE. It is expressed in the same units as the output variable, which makes it more interpretable.\n\n\\begin{align}\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} = \\sqrt{\\text{MSE}}\\end{align}\n\n#### Coefficient of Determination or Score ($R^2$)\n\nThe $R^2$ measures the proportion of the variance of the dependent or predicted variable against the independent variables. Normally it ranges from 0 (not fit at all) to 1 (perfect fit). A rare negative value indicates the prediction model is worse than a prediction with the mean.\n\n\\begin{align}R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\end{align}\n\nWhere:\n\n- $\\bar{y}$ is the mean of the real values $y_i$.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}