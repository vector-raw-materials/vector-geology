{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\nfrom pyvista import set_plot_theme\nset_plot_theme('document')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiphysics property prediction from hyperspectral drill core data\nThis notebook uses drill core data to train a model that predicts petrophysical properties from hyperspectral data.j\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dotenv\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nfrom sklearn.cluster import HDBSCAN\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom tqdm import tqdm\n\nimport hklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have prepared a Stack object with the hyperspectral and petrophysical data integrated into it\nLoad the Stack\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dotenv.load_dotenv()\nbase_path = os.getenv(\"PATH_TO_HyTorch\")\n\nS = hklearn.Stack.load(f\"{base_path}/Training_Stack\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the spectra and properties (hklearn filters out the NaNs)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = S.X() # Spectra\ny = S.y() # Properties and their standard deviations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a single spectrum\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(4, 3))\nplt.plot(S.get_wavelengths(\"SWIR\")/1e3, S.X(\"SWIR\")[550])\nplt.plot(S.get_wavelengths(\"MWIR\")/1e3, S.X(\"MWIR\")[550])\nplt.plot(S.get_wavelengths(\"LWIR\")/1e3, S.X(\"LWIR\")[550])\nplt.xlabel(r\"Wavelength $(\\mu m)$\")\nplt.legend([\"VNIR-SWIR\", \"MWIR\", \"LWIR\"])\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Filtering\nWe do two steps of filtering:\n1. We use the standard deviations to eliminate points with lithological contacts.\n2. We use HDBSCAN to generate clusters based on the PCA of the spectra, which eliminates 'noisy' spectra that aren't spectrally abundant\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "High variance filtering\nRemove the high variance points (Using the rolling standard deviations)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "keep_idx = np.logical_and(S.y()[:, 4] < 5, np.logical_and(S.y()[:, 5] < 5e-2, S.y()[:, -1] < 1000))\nX = X[keep_idx]\ny = y[keep_idx, :4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clustering\nFit a PCA\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hylite.filter import PCA\npca, loadings, _ = PCA(X, bands=30)\npca.data = pca.data/np.max(np.abs(pca.data), axis=0)[None, :]\n\n# Init\nclustering = HDBSCAN(10, 10)\n\n# Fit + Predict\nlabels = clustering.fit_predict(np.c_[y[:, 0] * 1e-2, pca.data])\nun_l, un_cts = np.unique(labels, return_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using `matplotlib.pyplot` to visualize the effect of the filtering and clustering\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the properties\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplot_mosaic([['A)', 'B)', 'C)']], layout='constrained', sharey=True, sharex=True, figsize=(8, 4))\n\n# Original Data\nlabel = list(axs.keys())\nax = list(axs.values())\nm = ax[0].scatter(S.y()[:, 1], S.y()[:, 2], c=S.y()[:, 0]/1e3, s=3)\nax[0].set_title(r\"Original Data\" + \"\\n\" + r\"$(N = %d)$\" % S.y().shape[0])\nax[0].set_ylabel(r\"Density $(g.cm^{-3})$\")\nax[0].set_title(label[0], loc='left', fontsize='medium')\ncbaxes = inset_axes(ax[0], width=\"3%\", height=\"37%\", loc=3)\ncbaxes.tick_params(labelsize=8)\nplt.colorbar(cax=cbaxes, mappable=m)\ncbaxes.set_ylabel(r\"Depths $(km)$\", fontsize=8)\n\n# Cleaned data\nm1 = ax[1].scatter(y[:, 1], y[:, 2], c=y[:, -1], s=3, cmap=\"cool\")\nax[1].set_title(r\"Cleaned Data\" + \"\\n\" + r\"$(N = %d)$\" % y.shape[0])\nax[1].set_xlabel(r\"Slowness $(\\mu s.m^{-1})$\")\nax[1].set_title(label[1], loc='left', fontsize='medium')\ncbaxes = inset_axes(ax[1], width=\"3%\", height=\"37%\", loc=3)\ncbaxes.tick_params(labelsize=8)\nplt.colorbar(cax=cbaxes, mappable=m1)\ncbaxes.set_ylabel(r\"$\\gamma$ $(API)$\", fontsize=8)\n\n# Labeled data\nm2 = ax[2].scatter(y[labels >= 0., 1], y[labels >= 0., 2], c=labels[labels >= 0.], s=3, cmap=\"turbo\")\nax[2].set_title(r\"Clustered Data\" + \"\\n\" + r\"$(N = %d, N_c = %d)$\" % (np.sum(labels >= 0.), un_l.shape[0] - 1))\nax[2].set_title(label[2], loc='left', fontsize='medium')\ncbaxes = inset_axes(ax[2], width=\"3%\", height=\"37%\", loc=3)\ncbaxes.tick_params(labelsize=8)\nplt.colorbar(cax=cbaxes, mappable=m2)\ncbaxes.set_ylabel(r\"Class\", fontsize=8)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract the hyperspectral data\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the labeled data (Drop the NaNs)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fin_idx = labels >= 0.\n# Complete spectrum\nfin_X = 1 - S.X()[keep_idx][fin_idx]\n# SWIR\nfin_swir = 1 - S.X(sensor=\"SWIR\")[keep_idx][fin_idx]\n# MWIR\nfin_mwir = 1 - S.X(sensor=\"MWIR\")[keep_idx][fin_idx]\n# LWIR\nfin_lwir = 1 - S.X(sensor=\"LWIR\")[keep_idx][fin_idx]\n# Scale the properties to keep the order of magnitude the same\nfin_y = S.y()[keep_idx][fin_idx, 1:4] * np.array([1e-3, 1e-1, 1e-3])[None, :]\n# Labels\nfin_lbls = labels[fin_idx].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define a shuffled Train + Validation split\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use stratified shuffle splitting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_splits = 6\ntest_size = 0.25\nsss = StratifiedShuffleSplit(n_splits=n_splits,\n                             test_size=test_size,\n                             random_state=404)\n\nidxs = np.arange(fin_lbls.shape[0])\ntrain_idxs = []\nvalid_idxs = []\n\nfor train_idx, valid_idx in sss.split(idxs, fin_lbls):\n    train_idxs.append(train_idx)\n    valid_idxs.append(valid_idx)\n    \n# Stack\ntrain_idxs = np.vstack(train_idxs)\nvalid_idxs = np.vstack(valid_idxs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Define a `pytorch` model\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Torch\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torcheval.metrics import R2Score, MeanSquaredError\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\n\n# Classes\n# Dataset\nclass MultimodalDataset(Dataset):\n    def __init__(self, swir, mwir, lwir, labels, targets):\n        self.swir = swir\n        self.mwir = mwir\n        self.lwir = lwir\n        self.labels = labels\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        return self.swir[idx], self.mwir[idx], self.lwir[idx], self.labels[idx], self.targets[idx]\n\n\nclass WeightedMSELoss(nn.Module):\n    def __init__(self, non_neg_penalty_weight=1.0):\n        super(WeightedMSELoss, self).__init__()\n        self.non_neg_penalty_weight = non_neg_penalty_weight\n\n    def forward(self, inputs, weights, targets):\n        # Calculate the MSE loss for each example in the batch\n        mse_loss = (inputs - targets) ** 2\n        # Apply weights to the MSE loss\n        weighted_mse_loss = mse_loss * weights[:, None]\n        # Calculate the mean loss\n        loss = weighted_mse_loss.mean()\n        \n        # Add non-negativity penalty\n        non_neg_penalty = self.non_neg_penalty_weight * torch.sum(torch.clamp(-inputs, min=0) ** 2)\n        total_loss = loss + non_neg_penalty\n        \n        return total_loss\n\nclass MultiHeadedMLP(nn.Module):\n    def __init__(self, in_sizes, hidden_sizes, out_channels, output_size, conv_kernel_size=[3, 3, 3], conv_stride=1, conv_padding=1):\n        super(MultiHeadedMLP, self).__init__()\n        \n        # Calculate output sizes after convolution\n        self.swir_conv_output_size = self._calculate_conv_output_size(in_sizes[0], conv_kernel_size[0], conv_stride, conv_padding)\n        self.mwir_conv_output_size = self._calculate_conv_output_size(in_sizes[1], conv_kernel_size[1], conv_stride, conv_padding)\n        self.lwir_conv_output_size = self._calculate_conv_output_size(in_sizes[2], conv_kernel_size[2], conv_stride, conv_padding)\n        \n        # Define separate input heads for each band type with a conv layer\n        self.swir_head = nn.Sequential(\n            nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=conv_kernel_size[0], stride=conv_stride, padding=conv_padding),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(self.swir_conv_output_size * out_channels, hidden_sizes[0]),\n            nn.ReLU(),\n        )\n        self.mwir_head = nn.Sequential(\n            nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=conv_kernel_size[1], stride=conv_stride, padding=conv_padding),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(self.mwir_conv_output_size * out_channels, hidden_sizes[1]),\n            nn.ReLU(),\n        )\n        self.lwir_head = nn.Sequential(\n            nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=conv_kernel_size[2], stride=conv_stride, padding=conv_padding),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(self.lwir_conv_output_size * out_channels, hidden_sizes[2]),\n            nn.ReLU(),\n        )\n        \n        # Define a shared hidden layer after combining the inputs\n        combined_input_size = hidden_sizes[0] + hidden_sizes[1] + hidden_sizes[2]\n        self.shared_layer = nn.Sequential(\n            nn.Linear(combined_input_size, combined_input_size * 2),\n            nn.ReLU(),\n            nn.Linear(combined_input_size * 2, combined_input_size // 2),\n            nn.ReLU(),\n            nn.Linear(combined_input_size // 2, 16),\n            nn.ReLU(),\n            nn.Linear(16, output_size),\n        )\n    \n    def _calculate_conv_output_size(self, input_size, kernel_size, stride, padding):\n        return (input_size - kernel_size + 2 * padding) // stride + 1\n    \n    def forward(self, swir, mwir, lwir):\n        # Add channel dimension for conv layer\n        swir = swir.unsqueeze(1)\n        mwir = mwir.unsqueeze(1)\n        lwir = lwir.unsqueeze(1)\n        \n        swir_out = self.swir_head(swir)\n        mwir_out = self.mwir_head(mwir)\n        lwir_out = self.lwir_head(lwir)\n\n        # Concatenate the outputs from each head\n        combined = torch.cat((swir_out, mwir_out, lwir_out), dim=1)\n\n        # Pass through the shared layer\n        output = self.shared_layer(combined)\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the model and prepare for training\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make datasets\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n\n# Initialize a model\nhidden_sizes = [32, 32, 32]\nin_sizes = [S.X(sensor).shape[1] for sensor in S.get_sensors()]\noutput_size = 3\nconv_kernel_size = [60, 40, 20]\nconv_stride = 1\nconv_padding = 1\nout_channels = 4\n\nmodel = MultiHeadedMLP(in_sizes, hidden_sizes,\n                       out_channels, output_size,\n                       conv_kernel_size, conv_stride,\n                       conv_padding)\n\n# Loss Function\nwt_loss_fn = WeightedMSELoss(non_neg_penalty_weight=2)\nloss_fn = nn.MSELoss()\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Number of training epochs (Per fold)\nn_epochs = 100\n \n# Initialize parameters\nbest_mse = np.inf\nbest_weights = None\ntrain_history = []\nhistory = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Begin Training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for j in range(n_splits):  \n    # Fold training\n    train_idx = train_idxs[j]\n    # Fold Validation\n    valid_idx = valid_idxs[j]\n    \n    # Get the separated datasets\n    # Training\n    train_X, train_swir, train_mwir, train_lwir, train_y = torch.Tensor(fin_X[train_idx]), torch.Tensor(fin_swir[train_idx]), torch.Tensor(fin_mwir[train_idx]), torch.Tensor(fin_lwir[train_idx]), torch.Tensor(fin_y[train_idx])\n    # Validation\n    valid_X, valid_swir, valid_mwir, valid_lwir, valid_y = torch.Tensor(fin_X[valid_idx]), torch.Tensor(fin_swir[valid_idx]), torch.Tensor(fin_mwir[valid_idx]), torch.Tensor(fin_lwir[valid_idx]), torch.Tensor(fin_y[valid_idx])\n\n    # Compute the weights\n    fold_idxs = [train_idx, valid_idx]\n    weights = []\n\n    for i in range(2):\n        # Define the weights\n        lbls, counts = np.unique(fin_lbls[fold_idxs[i]], return_counts=True)\n        counts = 1/counts\n        class_weights = counts/counts.sum()\n        # Assign the weights\n        loss_weights = np.array([class_weights[fin_lbls[i] == lbls] for i in range(fin_lbls[fold_idxs[i]].shape[0])])\n        weights.append(torch.Tensor(loss_weights))\n\n    train_dataset = MultimodalDataset(train_swir, train_mwir, train_lwir, weights[0], train_y)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    \n    with tqdm(range(n_epochs), unit=\" epochs\", mininterval=0, disable=False) as bar_:\n        bar_.set_description(f\"Training Fold {j + 1}\")\n\n        for epoch in bar_:\n            model.train()\n\n            with tqdm(train_loader, unit=\"batch\", mininterval=0, disable=True) as bar:\n                bar.set_description(f\"Epoch {epoch}\")\n                for batch_swir, batch_mwir, batch_lwir, batch_weights, y_batch in bar:\n\n                    # Forward pass\n                    y_pred = model(batch_swir, batch_mwir, batch_lwir)\n\n                    # Calculate Loss\n                    loss = wt_loss_fn(y_pred, batch_weights, y_batch)\n\n                    # Backward pass\n                    optimizer.zero_grad()\n                    loss.backward()\n\n                    # Update weights\n                    optimizer.step()\n\n            # Log training loss for the epoch\n            train_pred = model(train_swir, train_mwir, train_lwir)\n            train_mse = loss_fn(train_pred, train_y)\n            train_history.append(train_mse.item())\n\n            # Validation Loss\n            valid_pred = model(valid_swir, valid_mwir, valid_lwir)\n            mse = loss_fn(valid_pred, valid_y)\n            history.append(mse.item())\n\n            if mse.item() < best_mse:\n                best_mse = mse.item()\n                best_weights = copy.deepcopy(model.state_dict())\n\n            # Print progress\n            bar_.set_postfix({\"Training Loss\" : train_mse.item(), \"Validation Loss\": mse.item(), \"Best Loss\": best_mse})\n        \n# Restore model with best weights\nmodel.load_state_dict(best_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Compare the predictions\nTo ensure reproducibility, we have included the best performing model from our study.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the pre-trained model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(f\"{base_path}/KSL_MultiFold_SixFold.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the predictions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_swir = torch.Tensor(fin_swir)\ninput_mwir = torch.Tensor(fin_mwir)\ninput_lwir = torch.Tensor(fin_lwir)\n\n# Ensure the model is in evaluation mode\nmodel.eval()\n\n# Run the forward pass - no need to track gradients here\nwith torch.no_grad():\n    predictions = model(input_swir, input_mwir, input_lwir)\n    \n# Inverse scaler\nmeas_ = fin_y * np.array([1e3, 1e1, 1e3])[None, :]\npred_ = predictions * np.array([1e3, 1e1, 1e3])[None, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot scatter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lowlims = [130, 1.75, -10]\nhighlims = [320, 3.4, 200]\ntitles = [r\"Slowness\", r\"Density\", r\"Gamma-Ray\"]\nunits = [r\"$\\ \\mu \\mathrm{s.m^{-1}}$\", r\"$\\ \\mathrm{g.cm^{-3}}$\", \"$\\ \\mathrm{API}$\"]\n\nfig, axs = plt.subplot_mosaic([['A)', 'B)', 'C)']], layout='constrained', figsize=(6, 2.2))\nprops = dict(boxstyle='round', facecolor='lightblue', edgecolor=\"lightblue\", alpha=0.5)\n\n# Original Data\nlabel = list(axs.keys())\ncax = list(axs.values())\n\nfor i in range(pred_.shape[1]):\n    meas = meas_[:, i]\n    pred = pred_[:, i]\n    ax = cax[i]\n    \n    # Compute Metric\n    metric = MeanSquaredError()\n    metricr2 = R2Score()\n    metric.update(torch.Tensor(meas), torch.Tensor(pred))\n    metricr2.update(torch.Tensor(meas), torch.Tensor(pred))\n    rmse = np.sqrt(metric.compute().item())\n    r2 = metricr2.compute().item()\n    \n    ax.scatter(meas, pred, s=3)\n    ax.set_title(label[i], loc='left', fontsize='medium')\n    ax.set_title(titles[i])\n    \n    textstr = '\\n'.join((\n    r'$R^2=%.3f$' % (r2, ),\n    r'$RMSE=%.3f$' % (rmse, ),\n    ))\n        \n    # place a text box in upper left in axes coords\n    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=7,\n            verticalalignment='top', bbox=props)\n    \n    ax.axline((0, 0), slope=1, c=\"r\")\n    ax.set_xlim([lowlims[i], highlims[i]])\n    ax.set_ylim([lowlims[i], highlims[i]])\n    ax.set_xlabel(\"Measured\")\n    if i == 0:\n        ax.set_ylabel(\"Predicted\")\n    ax.set_aspect(\"equal\")\n\nplt.show()\n\n# sphinx_gallery_thumbnail_number = -1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}